# -*- coding: utf-8 -*-
"""NLP Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1K1xbZN40uIWEIa4wkbK40bmYHzhs6Pdd

# Import libraries

- download stopwords, punkt, and wordnet
- stopwords is for import stopwords
- punkt is for import word_tokenize
- wordnet is for import WordNetLemmatizer
"""

import tensorflow as tf
import pandas as pd
import nltk
from sklearn.model_selection import train_test_split
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')

"""# Load dataset"""

df = pd.read_csv('Stress.csv')
df.head(10)

"""# Data cleaning
- remove stopwords
- lemmatization
- remove punctuation

```
df['text'] = [word_tokenize(text) for text in df['text']]

for index,text in enumerate(df['text']):
  df['text'][index] = [word for word in text if word.lower() not in stop_words]

```


"""

# tokenize words
df['text'] = df['text'].apply(word_tokenize)

# remove stopwords
stop_words = set(stopwords.words('english'))
df['text'] = df['text'].apply(lambda text: [word.lower() for word in text if word.lower() not in stop_words])

# implement lemmatization
# comment this if you want to
lemmatizer = WordNetLemmatizer()
df['text'] = df['text'].apply(lambda text: [lemmatizer.lemmatize(word) for word in text])

# remove punctuation
df['text'] = df['text'].apply(lambda text: [word for word in text if word.isalnum()])
print(df['text'])

"""# Split dataset
in this dataset there is a column named text and label
- text : what people said in each subreddit
- label : indicator if someone is stressed based by the text (0 -> not stressed, 1 -> stressed)
"""

kalimat = df['text'].values
label = df['label'].values
print(kalimat[0], "\n", label[0])

"""# Split data into train and test

- random_state is used so that you will get the same result as i do
"""

kalimat_latih, kalimat_test, label_latih, label_test = train_test_split(kalimat, label, test_size=0.2, random_state=42)

"""# Tokenization"""

tokenizer = Tokenizer(num_words=150000, oov_token='x')
tokenizer.fit_on_texts(kalimat_latih)

sekuens_latih = tokenizer.texts_to_sequences(kalimat_latih)
sekuens_test = tokenizer.texts_to_sequences(kalimat_test)

padded_latih = pad_sequences(sekuens_latih)
padded_test = pad_sequences(sekuens_test)

print(f"Kalimat latih: {kalimat_latih} \n")
print(f"Sekuens latih: {sekuens_latih} \n")
print(f"Padded latih: {padded_latih} \n")